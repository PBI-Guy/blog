{"cells":[{"cell_type":"code","execution_count":null,"id":"0ae25e3e-e694-4108-8fce-12f2205837dd","metadata":{"jupyter":{"outputs_hidden":true}},"outputs":[],"source":["#Import required libraries\n","\n","%pip install semantic-link\n","%load_ext sempy\n","import pandas as pd\n","import sempy.fabric as fabric\n","from datetime import date, datetime, timedelta\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampNTZType\n","\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\n","spark.conf.set(\"spark.sql.catalog.pbi\", \"com.microsoft.azure.synapse.ml.powerbi.PowerBICatalog\")"]},{"cell_type":"code","execution_count":null,"id":"ad99c4a3-1c5e-4894-99f2-43d9e8e3d7ec","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Define required parameters\n","dataset = 'Fabric Capacity Metrics' #Default name of the Semantic Model\n","talbe_name = 'Capacities' #Table Name to get all Capacities\n","today = datetime.today() #Get Today's date\n","timepoint_start = today - timedelta(days=1) #Get yesterday's date on which we will extract all details\n","path_bronze = 'Files/01 Bronze/'\n","path_silver = 'Files/02 Silver/'\n","path_gold = 'Files/03 Gold/'\n","dataframe_schema = StructType([\n","                StructField(\"BillingType\", StringType(), nullable=True),\n","                StructField(\"Status\", StringType(), nullable=True),\n","                StructField(\"OperationStartTime\", TimestampNTZType(), nullable=True),\n","                StructField(\"OperationEndTime\", TimestampNTZType(), nullable=True),\n","                StructField(\"User\", StringType(), nullable=True),\n","                StructField(\"Operation\", StringType(), nullable=True),\n","                StructField(\"OperationID\", StringType(), nullable=True),\n","                StructField(\"WorkspaceName\", StringType(), nullable=True),\n","                StructField(\"Item\", StringType(), nullable=True),\n","                StructField(\"ItemName\", StringType(), nullable=True),\n","                StructField(\"TimepointCUs\", FloatType(), nullable=True),\n","                StructField(\"DurationInS\", IntegerType(), nullable=True),\n","                StructField(\"TotalCUInS\", FloatType(), nullable=True),\n","                StructField(\"Throttling\", IntegerType(), nullable=True),\n","                StructField(\"PercentageOfBaseCapacity\", FloatType(), nullable=True),\n","                StructField(\"capacityId\", StringType(), nullable=True),\n","                StructField(\"Timepoint\", TimestampNTZType(), nullable=True),\n","                StructField(\"OperationType\", StringType(), nullable=True)\n","            ])"]},{"cell_type":"code","execution_count":null,"id":"ffaec11f-5e5d-48c1-aee8-d1a43e8451f8","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Get all capacities\n","df_capacities = spark.sql(\"\"\"\n","    SELECT c.capacityId\n","\n","    FROM pbi.`\"\"\" + dataset + \"\"\"`.Capacities c\n","\"\"\")"]},{"cell_type":"code","execution_count":null,"id":"3589ccbf-e0a9-463b-9f18-758b52c42428","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#For testing purpose filtering it down to only one capacity\n","capacity_id = '...'\n","df_capacities = df_capacities[df_capacities['capacityId'] == capacity_id]\n","\n","display(df_capacities)"]},{"cell_type":"code","execution_count":null,"id":"3f375e62-b35e-44e0-b52b-18edcb5ae307","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Create a function to get Background Operations of the Metrics App\n","\n","def generate_dax_background_operation(date_today, capacity_id):\n","    \"\"\"\n","    Generate the DAX statement which is used to get the background operations of the Metrics App for a given Capacity and day. \n","    \n","    Arguments required: \n","        date_today (datetime) - Date on which the background operation should be extracted\n","        capacity_id (string) - Capacity ID on which the background operation should be extracted\n","\n","    Returns:\n","        DAX Statement (string)\n","\n","    \"\"\"\n","\n","    #timepoint_start = date_today.replace(hour=0, minute=0, second=0, microsecond=0) #Set timepoint to the beginning of the day\n","    timepoint_start = date_today.replace(day=24, month=4, year=2024, hour=9, minute=00, second=00, microsecond=00) #Use this timepoint to get a specific one - used for testing purpose\n","    timepoint_next = timepoint_start\n","    i = 0 #Initialising iteration count to check if all timepoints (2880 in total for a day) has been covered\n","\n","    #while timepoint_next.day == timepoint_start.day: #As long as the day of the next timepoint is the same as start timepiont, loop will continue and add 30seconds at the end\n","    while timepoint_next <= datetime.strptime('24.04.2024 09:02:00', \"%d.%m.%Y %H:%M:%S\"): #Use this filter to get some specific timepoints only - used for testing purpose\n","        current_year = str(timepoint_next.year)\n","        current_month = str(timepoint_next.month)\n","        current_day = str(timepoint_next.day)\n","        starting_hour = str(timepoint_next.hour)\n","        starting_minutes = str(timepoint_next.minute)\n","        starting_seconds = str(timepoint_next.second)\n","\n","        dax_background_operation = '''\n","                    DEFINE\n","                        MPARAMETER 'CapacityID' = \"''' + capacity_id + '''\"\n","                        MPARAMETER 'TimePoint' = (DATE(''' + current_year + ''', ''' + current_month + ''', ''' + current_day + ''') + TIME(''' + starting_hour + ''', ''' + starting_minutes + ''', ''' + starting_seconds + '''))\n","\n","                        VAR varFilter_Capacity = TREATAS({\"''' + capacity_id + '''\"}, 'Capacities'[capacityId])\t\n","                        VAR varFilter_TimePoint = \n","                            TREATAS(\n","                                {(DATE(''' + current_year + ''', ''' + current_month + ''', ''' + current_day + ''') + TIME(''' + starting_hour + ''', ''' + starting_minutes + ''', ''' + starting_seconds + '''))},\n","                                'TimePoints'[TimePoint]\n","                            )\t\t\n","                        VAR varTable_Details =\n","                            SUMMARIZECOLUMNS(\n","                                'TimePointBackgroundDetail'[OperationStartTime],\n","                                'TimePointBackgroundDetail'[OperationEndTime],\n","                                'TimePointBackgroundDetail'[Status],\n","                                'TimePointBackgroundDetail'[Operation],\n","                                'TimePointBackgroundDetail'[User],\n","                                'TimePointBackgroundDetail'[OperationId],\n","                                'TimePointBackgroundDetail'[Billing type],\n","                                'Items'[WorkspaceName],\n","                                'Items'[ItemKind],\n","                                'Items'[ItemName],\n","                                \n","                                varFilter_Capacity,\n","                                varFilter_TimePoint,\n","                                \n","                                \"Timepoint CU (s)\", SUM('TimePointBackgroundDetail'[Timepoint CU (s)]),\n","                                \"Duration (s)\", SUM('TimePointBackgroundDetail'[Duration (s)]),\n","                                \"Total CU (s)\", CALCULATE(SUM('TimePointBackgroundDetail'[Total CU (s)])),\n","                                \"Throttling\", CALCULATE(SUM('TimePointBackgroundDetail'[Throttling (s)])),\n","                                \"% of Base Capacity\", CALCULATE(SUM('TimePointBackgroundDetail'[% of Base Capacity]))\n","                            )\n","                                \n","                    EVALUATE  SELECTCOLUMNS(\n","                        varTable_Details,\n","                        \"BillingType\", [Billing type],\n","                        \"Status\", [Status],\n","                        \"OperationStartTime\", [OperationStartTime],\n","                        \"OperationEndTime\", [OperationEndTime],\n","                        \"User\", [User],\n","                        \"Operation\", [Operation],\n","                        \"OperationID\", [OperationId],\n","                        \"WorkspaceName\", [WorkspaceName],\n","                        \"Item\", [ItemKind],\n","                        \"ItemName\", [ItemName],\n","                        \"TimepointCUs\", [Timepoint CU (s)],\n","                        \"DurationInS\", [Duration (s)],\n","                        \"TotalCUInS\", [Total CU (s)],\n","                        \"Throttling\", [Throttling],\n","                        \"PercentageOfBaseCapacity\", [% of Base Capacity]\t\n","                    )'''\n","        \n","        df_dax_result = fabric.evaluate_dax(\n","            dataset,\n","            dax_background_operation\n","            )\n","\n","        if not df_dax_result.empty:\n","        \n","            df_dax_result['capacityId'] = capacity_id\n","            df_dax_result['Timepoint'] = timepoint_next\n","            df_dax_result['OperationType'] = 'background'\n","\n","            #Set path and file name\n","            subfolder = str(timepoint_next.date()) + '/'\n","            file_name = timepoint_next.strftime(\"%H-%M-%S\")\n","\n","            #Convert Fabric DataFrames into Spark DataFrames\n","            df_dax_result_spark = spark.createDataFrame(df_dax_result, schema=dataframe_schema)\n","\n","            #Save DataFrames to OneLake\n","            df_dax_result_spark.write.mode(\"overwrite\").format(\"parquet\").save(path_bronze + 'Background Operation/' + subfolder + file_name)\n","            \n","        #Don't change timepoint intervals, as 30sec intervals are given\n","        timepoint_next = timepoint_next + timedelta(seconds = 30)\n","\n","        i = i + 1\n","\n","    return i"]},{"cell_type":"code","execution_count":null,"id":"d8fb4491-1d98-4187-8932-ee5439abfbbb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Create a function to get Interactive Operations of the Metrics App\n","\n","def generate_dax_interactive_operation(date_today, capacity_id):\n","    \"\"\"\n","    Generate the DAX statement which is used to get the interactive operations of the Metrics App for a given Capacity and day. \n","    \n","    Arguments required: \n","        date_today (datetime) - Date on which the interactive operation should be extracted\n","        capacity_id (string) - Capacity ID on which the interactive operation should be extracted\n","\n","    Returns:\n","        DAX Statement (Pandas DataFrame)\n","\n","    \"\"\"\n","\n","    #timepoint_start = date_today.replace(hour=0, minute=0, second=0, microsecond=0) #Set timepoint to the beginning of the day\n","    timepoint_start = date_today.replace(day=24, month=4, year=2024, hour=9, minute=00, second=00, microsecond=00) #Use this timepoint to get a specific one - used for testing purpose\n","    timepoint_next = timepoint_start\n","    i = 0 #Initialising iteration count to check if all timepoints (2880 in total for a day) has been covered\n","\n","    #while timepoint_next.day == timepoint_start.day: #As long as the day of the next timepoint is the same as start timepoint, loop will continue and add 30seconds at the end\n","    while timepoint_next <= datetime.strptime('24.04.2024 09:02:00', \"%d.%m.%Y %H:%M:%S\"): #Use this filter to get some specific timepoints only - used for testing purpose\n","        current_year = str(timepoint_next.year)\n","        current_month = str(timepoint_next.month)\n","        current_day = str(timepoint_next.day)\n","        starting_hour = str(timepoint_next.hour)\n","        starting_minutes = str(timepoint_next.minute)\n","        starting_seconds = str(timepoint_next.second)\n","\n","        dax_interactive_operation = '''\n","                    DEFINE\n","                        MPARAMETER 'CapacityID' = \"''' + capacity_id + '''\"\n","                        MPARAMETER 'TimePoint' = (DATE(''' + current_year + ''', ''' + current_month + ''', ''' + current_day + ''') + TIME(''' + starting_hour + ''', ''' + starting_minutes + ''', ''' + starting_seconds + '''))\n","\n","                        VAR varFilter_Capacity = TREATAS({\"''' + capacity_id + '''\"}, 'Capacities'[capacityId])\t\n","                        VAR varFilter_TimePoint = \n","                            TREATAS(\n","                                {(DATE(''' + current_year + ''', ''' + current_month + ''', ''' + current_day + ''') + TIME(''' + starting_hour + ''', ''' + starting_minutes + ''', ''' + starting_seconds + '''))},\n","                                'TimePoints'[TimePoint]\n","                            )\t\t\n","                        VAR varTable_Details =\n","                            SUMMARIZECOLUMNS(\n","                                'TimePointInteractiveDetail'[OperationStartTime],\n","                                'TimePointInteractiveDetail'[OperationEndTime],\n","                                'TimePointInteractiveDetail'[Status],\n","                                'TimePointInteractiveDetail'[Operation],\n","                                'TimePointInteractiveDetail'[User],\n","                                'TimePointInteractiveDetail'[OperationId],\n","                                'TimePointInteractiveDetail'[Billing type],\n","                                'Items'[WorkspaceName],\n","                                'Items'[ItemKind],\n","                                'Items'[ItemName],\n","                                \n","                                varFilter_Capacity,\n","                                varFilter_TimePoint,\n","                                \n","                                \"Timepoint CU (s)\", SUM('TimePointInteractiveDetail'[Timepoint CU (s)]),\n","                                \"Duration (s)\", SUM('TimePointInteractiveDetail'[Duration (s)]),\n","                                \"Total CU (s)\", CALCULATE(SUM('TimePointInteractiveDetail'[Total CU (s)])),\n","                                \"Throttling\", CALCULATE(SUM('TimePointInteractiveDetail'[Throttling (s)])),\n","                                \"% of Base Capacity\", CALCULATE(SUM('TimePointInteractiveDetail'[% of Base Capacity]))\n","                            )\n","                                \n","                    EVALUATE  SELECTCOLUMNS(\n","                        varTable_Details,\n","                        \"BillingType\", [Billing type],\n","                        \"Status\", [Status],\n","                        \"OperationStartTime\", [OperationStartTime],\n","                        \"OperationEndTime\", [OperationEndTime],\n","                        \"User\", [User],\n","                        \"Operation\", [Operation],\n","                        \"OperationID\", [OperationId],\n","                        \"WorkspaceName\", [WorkspaceName],\n","                        \"Item\", [ItemKind],\n","                        \"ItemName\", [ItemName],\n","                        \"TimepointCUs\", [Timepoint CU (s)],\n","                        \"DurationInS\", [Duration (s)],\n","                        \"TotalCUInS\", [Total CU (s)],\n","                        \"Throttling\", [Throttling],\n","                        \"PercentageOfBaseCapacity\", [% of Base Capacity]\t\n","                    )'''\n","        \n","        df_dax_result = fabric.evaluate_dax(\n","            dataset,\n","            dax_interactive_operation\n","            )\n","        \n","        if not df_dax_result.empty:\n","\n","            df_dax_result['capacityId'] = capacity_id\n","            df_dax_result['Timepoint'] = timepoint_next\n","            df_dax_result['OperationType'] = 'interactive'\n","            \n","            #Set path and file name\n","            subfolder = str(timepoint_next.date()) + '/'\n","            file_name = timepoint_next.strftime(\"%H-%M-%S\")\n","\n","            #Convert Fabric DataFrames into Spark DataFrames\n","            df_dax_result_spark = spark.createDataFrame(df_dax_result, schema=dataframe_schema)\n","\n","            #Save DataFrames to OneLake\n","            df_dax_result_spark.write.mode(\"overwrite\").format(\"parquet\").save(path_bronze + 'Interactive Operation/' + subfolder + file_name)\n","\n","        #print(i, timepoint_next, \"interactive\")\n","        \n","        #Don't change timepoint intervals, as 30sec intervals are given\n","        timepoint_next = timepoint_next + timedelta(seconds = 30)\n","\n","        i = i + 1\n","\n","    return i"]},{"cell_type":"code","execution_count":null,"id":"cc260d4b-c6db-46d7-88e8-ea68b46d8821","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Get for each capacity background and interactive operations\n","for row in df_capacities.toLocalIterator():\n","    capacity_id = row['capacityId']\n","    \n","    i_background = generate_dax_background_operation(timepoint_start, capacity_id)\n","    i_interactive = generate_dax_interactive_operation(timepoint_start, capacity_id)"]},{"cell_type":"code","execution_count":null,"id":"b8dba30c-e745-4f53-855f-836ef722d0a3","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Set Subfolder and file name\n","subfolder = str(timepoint_start.date()) + '/'\n","file_name = str(timepoint_start.date())\n","\n","#Read folder\n","df_background_bronze = spark.read.parquet(path_bronze + 'Background Operation/' + subfolder + '/*')\n","df_interactive_bronze = spark.read.parquet(path_bronze + 'Interactive Operation/' + subfolder + '/*')\n","\n","#Save DataFrames to OneLake Silver layer\n","df_background_bronze.write.mode(\"overwrite\").format(\"parquet\").save(path_silver + 'Background Operation/' + file_name)\n","df_interactive_bronze.write.mode(\"overwrite\").format(\"parquet\").save(path_silver + 'Interactive Operation/' + file_name)"]},{"cell_type":"code","execution_count":null,"id":"2b255bbc-baf4-46f6-a458-fd52c6f0d33f","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#Read folder from Silver layer\n","df_background_silver = spark.read.parquet(path_silver + 'Background Operation/' + file_name)\n","df_interactive_silver = spark.read.parquet(path_silver + 'Interactive Operation/' + file_name)\n","\n","#Combine background and interactive operations into one DataFrame\n","df_all_operations = df_background_silver.unionByName(df_interactive_silver)\n","\n","#Save DataFrame into Gold Layer of OneLake\n","df_all_operations.write.mode(\"overwrite\").format(\"delta\").save(path_gold + file_name)"]},{"cell_type":"code","execution_count":null,"id":"7ead8bc7-a4f8-42f9-80c6-db1d9476f01b","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df_all_operations_gold = spark.read.parquet(path_gold + file_name)\n","df_all_operations_gold.write.mode(\"append\").format(\"delta\").save('Tables/AllOperations')"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"b69b11c8-c99d-467f-a66a-3f8151e5bb39","default_lakehouse_name":"FabricMetricsApp","default_lakehouse_workspace_id":"d6c24620-07f2-4833-907a-89f5186f7c1c"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{"3ab49c70-ff63-4d17-8946-c9a131698c59":{"persist_state":{"view":{"chartOptions":{"aggregationType":"count","binsNumber":10,"categoryFieldKeys":["0"],"chartType":"bar","isStacked":false,"seriesFieldKeys":["0"],"wordFrequency":"-1"},"tableOptions":{},"type":"details"}},"sync_state":{"isSummary":false,"language":"scala","table":{"rows":[{"0":"9C3E7404-D2DA-4CB6-B93F-9873BDD3D95A","index":1}],"schema":[{"key":"0","name":"capacityId","type":"string"}],"truncated":false}},"type":"Synapse.DataFrame"}},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
